
Neural Network

Neural networks are a family of model architectures designed to find nonlinear patterns in data. 
During training of a neural network, the model automatically learns the optimal feature crosses to perform on the input data to minimize loss.

Activation fucntions: ReLu, sigmoid, tanh
Activation fucntions: enables neural networks to learn nonlinear relationships between features and the label.

In training a neural network:
Gradient descent finds the weights by minimizing the total loss.
Regularization changes the loss function by adding a penalty term, so gradient descent prefers simpler models.

Best practices for neural network training:
Vanishing gradient--ReLu
Exploding gradient-lowering learn rate, batch normalization
Dead ReLu units--LeakyReLu
Dropout regularization

Training using backpropagation

Multiclass classification

Two main variants: One vs all; one vs one (softmax)

One vs all: One-vs.-all provides a way to use binary classification for a series of yes or no predictions across multiple possible labels. sum of probability >1; as probability is determined independently of all other data sets.
One vs one: For one-vs.-one, we can instead apply a function called softmax, which assigns decimal probabilities to each class in a multi-class problem such that all probabilities add up to 1.0.

One vs all: Sigmoid; one vs one (Softmax)

Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:
You may not use softmax.
You must rely on multiple logistic regressions.

Embeddings

Encoding:  Encoding refers to the process of choosing an initial numerical representation of data to train the model on.
Problems: Number of data points, amount of computation, amount of memory, number of weights,
Solution: how to create embeddings, lower-dimensional representations of sparse data, that address these issues.

Embedding: An embedding is a vector representation of data in embedding space.

Embeddings will usually be specific to the task, and differ from each other when the task differs.
Embeddings are vectors that represent real-world objects like text, images and audio as points in continuous vector space.
It turns out that embeddings based on semantic similarity work well for many general language.

Word2vec: Technique fro creating vector representation of words.

Embeddings: Obtaining embeddings

Dimensionality reduction techniques
PCA: PCA tries to find highly correlated dimensions that can be collapsed into a single dimension.

An embedding layer translates the one-hot encoding into the three-dimensional embedding vector [2.98, -0.75, 0].

Contextual embeddings: Contextual embeddings allow a word to be represented by multiple embeddings that incorporate information about the surrounding words as well as the word itself. 
BERT, ELMo, GPT, Tranformer.

Embeddings = Understanding words
Model = Making decisions based on that understanding


LLM

A language model estimates the probability of a token or sequence of tokens occurring within a longer sequence of tokens. 
A token could be a word, a subword (a subset of a word), or even a single character.

An application can use the probability table to make predictions.

Estimating the probability of what fills in the blank in a text sequence can be extended to more complex tasks, including:

Generating text.
Translating text from one language to another.
Summarizing documents.

N-grams are ordered sequences of words used to build language models, where N is the number of words in the sequence.

Recurrent neural networks provide more context than N-grams. 
A recurrent neural network is a type of neural network that trains on a sequence of tokens. 

LLMs make much better predictions than N-gram language models or recurrent neural networks because:

LLMs contain far more parameters than recurrent models.
LLMs gather far more context.

Transformers are the state-of-the-art architecture for a wide variety of language model applications, such as translation:

Full transformers consist of an encoder and a decoder:

An encoder converts input text into an intermediate representation. An encoder is an enormous neural net.
A decoder converts that intermediate representation into useful text. A decoder is also an enormous neural net.

Transformers rely heavily on a concept called self-attention.The "self" in "self-attention" refers to the input sequence. 

After all, predicting a word or two is essentially the autocomplete feature built into various text, email, and authoring software. 
You might be wondering how LLMs can generate sentences or paragraphs or haikus about arbitrage.

LLMs: Fine-tuning, distillation, and prompt engineering



