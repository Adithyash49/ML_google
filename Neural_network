
Neural Network

Neural networks are a family of model architectures designed to find nonlinear patterns in data. 
During training of a neural network, the model automatically learns the optimal feature crosses to perform on the input data to minimize loss.

Activation fucntions: ReLu, sigmoid, tanh
Activation fucntions: enables neural networks to learn nonlinear relationships between features and the label.

In training a neural network:
Gradient descent finds the weights by minimizing the total loss.
Regularization changes the loss function by adding a penalty term, so gradient descent prefers simpler models.

Best practices for neural network training:
Vanishing gradient--ReLu
Exploding gradient-lowering learn rate, batch normalization
Dead ReLu units--LeakyReLu
Dropout regularization

Training using backpropagation

Multiclass classification

Two main variants: One vs all; one vs one (softmax)

One vs all: One-vs.-all provides a way to use binary classification for a series of yes or no predictions across multiple possible labels. sum of probability >1; as probability is determined independently of all other data sets.
One vs one: For one-vs.-one, we can instead apply a function called softmax, which assigns decimal probabilities to each class in a multi-class problem such that all probabilities add up to 1.0.

One vs all: Sigmoid; one vs one (Softmax)

Softmax assumes that each example is a member of exactly one class. Some examples, however, can simultaneously be a member of multiple classes. For such examples:
You may not use softmax.
You must rely on multiple logistic regressions.

Embeddings

Encoding:  Encoding refers to the process of choosing an initial numerical representation of data to train the model on.
Problems: Number of data points, amount of computation, amount of memory, number of weights,
Solution: how to create embeddings, lower-dimensional representations of sparse data, that address these issues.

Embedding: An embedding is a vector representation of data in embedding space.

Embeddings will usually be specific to the task, and differ from each other when the task differs.
Embeddings are vectors that represent real-world objects like text, images and audio as points in continuous vector space.
It turns out that embeddings based on semantic similarity work well for many general language.

Word2vec: Technique fro creating vector representation of words.

Embeddings: Obtaining embeddings

Dimensionality reduction techniques
PCA: PCA tries to find highly correlated dimensions that can be collapsed into a single dimension.

An embedding layer translates the one-hot encoding into the three-dimensional embedding vector [2.98, -0.75, 0].

Contextual embeddings: Contextual embeddings allow a word to be represented by multiple embeddings that incorporate information about the surrounding words as well as the word itself. 
BERT, ELMo, GPT, Tranformer.

Embeddings = Understanding words
Model = Making decisions based on that understanding


