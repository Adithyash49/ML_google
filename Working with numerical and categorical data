Working with numerical data

ML practitioners spend far more time evaluating, cleaning, and transforming data than building models.
Remember that your ML model interacts with the data in the feature vector, not the data in the dataset.

You must determine the best way to represent raw dataset values as trainable values in the feature vector. This process is called feature engineering

The most common feature engineering techniques are:

Normalization: Converting numerical values into a standard range.
Binning (also referred to as bucketing): Converting numerical values into buckets of ranges. In many cases binning turns numerical data into categorical data.

Numerical data : Step 1
Visualization, statistics

step 2: three popular normalization methods: better predictions, converge fast, Helps the model learn appropriate weights for each feature.

linear scaling:  (more commonly shortened to just scaling) means converting floating-point values from their natural range into a standard rangeâ€”usually 0 to 1 or -1 to +1.
Z-score scaling: A Z-score is the number of standard deviations a value is from the mean. A Z-score is the number of standard deviations a value is from the mean. 
log scaling: Log scaling changes the distribution, which helps train a model that will make better predictions.Log scaling computes the logarithm of the raw value.

Binning is a good alternative to scaling or clipping when either of the following conditions is met:

The overall linear relationship between the feature and the label is weak or nonexistent.
When the feature values are clustered.

Quantile bucketing creates bucketing boundaries such that the number of examples 
in each bucket is exactly or nearly equal. Quantile bucketing mostly hides the outliers.

Numerical data: Scrubbing

Problems: Omitted values, Duplicated examples, Out-of-range feature values, Bad labels

Polynomial transforms

Working with categorical data

Categorical data has a specific set of possible values. Numbers can also be categorical data(postal data).
Categorical data tends to produce high-dimensional feature vectors; that is, feature vectors having a large number of elements. 
High dimensionality increases training costs and makes training more difficult. 

Encoding

Encoding means converting categorical or other data to numerical vectors that a model can train on. This conversion is necessary because models can only train on floating-point values

Vocadbulary: The term dimension is a synonym for the number of elements in a feature vector.With a vocabulary encoding, the model treats each possible categorical value as a separate feature
Machine learning models can only manipulate floating-point numbers. Therefore, you must convert each string to a unique index number.

One-hot encoding: next step in building a vocabulary is to convert each index number to its one-hot encoding. 
Each category is represented by a vector (array) of N elements, where N is the number of categories.Exactly one of the elements 
in a one-hot vector has the value 1.0; all the remaining elements have the value 0.0.

It is the one-hot vector, not the string or the index number, that gets passed to the feature vector. 
The model learns a separate weight for each element of the feature vector.

If the features has high number of categories like postal code, words in english, last_names in country, then one hot vector is bad choice. 
So embeddings are preferred.
It reduces the dimensions and model runs quickly and infer predictions.

Hashing (also called the hashing trick) is a less common way to reduce the number of dimensions.

Categorical data: Common issues
Categorical data, on the other hand, is often categorized by human beings or by machine learning (ML) models

Any two human beings(gold labels) may label the same example differently. The difference between human raters' decisions is called inter-rater agreement.
To measure Inter rater agreement: Cohens kappa and variants, Intra class correlation, Krippendorff's alpha

Machine-labeled data, where categories are automatically determined by one or more classification models, is often referred to as silver labels

Feature cross: 
Feature crosses are created by crossing (taking the Cartesian product of) two or more categorical or bucketed features of the dataset
For any given example in the dataset, the feature cross will equal 1 only if both base features' original one-hot vectors were 1 for the crossed categories.
This dataset could be used to classify X(leaf) by Y(species) s, since these characteristics do not vary within a Y.



Datasets, generalization and overfitting

A dataset is a collection of examples.

Types of data: numerical, categorical, images, video, audio files, embeddings, documents

Following are common causes of unreliable data in datasets:Omitted values, duplicated values, bad feature values, bad labels, 

Imputation is the process of generating well-reasoned data, not random or deceptive data.
A sorted dataset, like the one in the following exercise, can sometimes simplify imputation. However, it is a bad idea to train on a sorted dataset. 
So, after imputation, randomize the order of examples in the training set.

Data labels: Direct and proxy labels

Datasets: Balanced and imbalanced

To overcome Imbalanced datasets following techniques are available : Downsampling and Upweighting

Datasets: Dividing the original dataset

Trainset, Validation set , testset

A validation set performs the initial testing on the model as it is being trained.
Use the validation set to evaluate results from the training set. After repeated use of 
the validation set suggests that your model is making good predictions, use the test set to double-check your model.

In summary, a good test set or validation set meets all of the following criteria:

Large enough to yield statistically significant testing results.
Representative of the dataset as a whole. In other words, don't pick a test set with different characteristics than the training set.
Representative of the real-world data that the model will encounter as part of its business purpose.
Zero examples duplicated in the training set.



