ML-based solutions are being increasingly used for predictive equipment maintenance, product quality assurance, process monitoring, fault diagnosis, process 
control, and process optimization, becoming integral part of life. Machine learning will continue to play significant role in unleashing the next wave of
productivity improvements in process industry. This practice of learning about systems from data or machine learning has become an indispensable tool in process operations in the age of
increasing global competition and stricter product quality standards. 

Process systems refer to a collection of physical structures that convert raw materials (wood, natural gas, crude oil, coal, etc.) into final consumer products (paper, fertilizers, diesel, energy,
etc.) or intermediate products which are then used to manufacture other commodity materials.Process industry is a broad
category that encompasses, amongst others, chemical industry, bioprocess industry, power industry, pharmaceuticals industry, steel industry, semiconductor industry, and waste
management industry.

In process industry, the task of optimizing production efficiency, controlling product quality, monitoring the process are categorized as process systems engineering (PSE) activities.
These tasks often require a mathematical model of the plant. The traditional practice has been to use first principles mathematical description of physio-chemical phenomena occurring 
inside each process unit to mathematically characterize the whole plant. However, as you may already know, building such fundamental models are time-consuming and difficult for
complex systems. Machine learning (ML)-based methods provide a convenient alternative where process data are used to build empirical plant model which can be used for
optimization, control, and monitoring purposes.

Characteristics of Process data

Industrial process data often exhibit characteristics which pose challenges to a process data scientist. Dyanamic, time-varying, batch vs continuous, multimode, discrete, non linear, high dimensionality.

What is ML?
At its core, machine learning simply means using computer programs and data for finding relationship between different components of a system and discovering patterns that were not immediately evident.
Domain expertise often helps to decide which ML algorithm to use and proper infusion of domain knowledge into ML methodology can sometimes increase model accuracy considerably.

Types of ML: 
Supervised: PCA, PLS, SVM, SVR, ANN, K-NN, LDA, Lasso, Ridge, Elastic, Decision Tree, Random forest. Application: Soft/virtual sensors, Predictive maintenance, Fault classification/diagnosis, process control, process monitoring
Unsupervised: PCA, Gaussian mixture model, DBSCAN, Kernel density estimation, K-means, SVDD, self organising map. Application: Dimensionality reduction,, Data visualization, denisty estimation, clustering, outlier detection
Reinforcement: 

Decision hierarchy levels in process plant:
Planning and logistics (weeks-months)
Production scheduling
Process diagnosis and supervision
Multivariable and constraint control, 
regulatory and primary control (seconds-minutes).

Partial least squares (PLS), principal component regression (PCR), support vector regression (SVR) are some of the popular ML method choices for estimating process quality variables.

Process industry is witnessing higher and higher product demands due to increasing population and growing lifestyle globally, but there is also a push to run production facilities
more efficiently and sustainably. Consequently, adoption of Industry 4.0, which mandates utilizing process data for process improvements all along the production chain, is on the rise.

Machine Learning Model Development:

Data pre-processing: Data cleaning-de-noising, variable selection, outlier handling, missing data handling; Data transformation - centering and scaling, Feature extraction, Feature engineering
Model training
Model evaluation - Evaluation metric selection, Cross validation, Residual analysis (model generalization)
Model tuning: Validation curvem regularization, K-fold CV (model hypertuning)

Automated feature engineering (AFE)00active research area in ML.

Workflow automation via pipelines; 

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression

Model Evaluation

Regression: MSE, MAE, RMSE, R2 ( R2measures how much of variability in the output variable can be explained by the model.)

Classification: Confusion matrix, Accuracy, Precision, Recall, F1 score, TPR, FPR

Holdout method / Cross-validation : K-fold; 
Residual analysis (Residual analysis is another way to validate the adequacy of (regression) models. It involves checking the model errors / residuals to ensure there are no undesired patterns in them.)

residual analysis is a powerful diagnostic tool. As a best practice, you should always check the residual plots to ensure no modeling assumptions have been violated or no
obvious room for improvement exists.

Model tuning (Hyperparameters)
Therefore, overfitting is characterized by small fitting errors and large validation errors. The sweet spot where the both
fitting and validation errors are small, and validation errors just start increasing corresponds to the optimal hyperparameters.

If you want to utilize all your training data for model fitting or validation subset cannot be set aside for model selection (e.g., when using gaussian mixture models), 
BIC or AIC technique can be used which provides an optimal trade-off between model complexity (or number of model parameters) and model accuracy.

Regularization: Lasso(L1), Ridge(L2)

Hyperparameter optimization via GridSearchCV
Previously we used validation curves to find the optimal hyperparameters. Sklearn provides a GridSearchCV utility function that we can leverage to do this task more conveniently.
GridSearchCV takes lists of potential hyperparameter values and does an exhaustive search to find the optimal combination of hyperparameter values. The code below shows the
implementation.


Data Pre-processing: Cleaning Process Data

Two common ways to de-noise process signals is to smooth them using moving window averages and Savitzky-Golay (SG) filtering.

Variable/Feature selection: 

Filter methods: Linear correlation (Pearson coefficient) and Mutual information
Wrapper methods: Recursive feature elimination (PLS) and sequential feature selection
Embedded methods: LASSO and Decision tree and random forest.(Embedded methods make use of algorithms where selection of variables is an inherent part
of the model fitting process itself.)

Outlier handling

Univariate method: 2 sigma rule, Hampel identifier
Multivariate methods: Mahalanobis distance, Minimum covariance determinant (MCD) estimator, PCA
Data-mining methods: KDE, GMM, K-means, LOF

Mahalanobis distance (MD) is a classical multivariate distance measure that takes into account the covariance/shape of data distribution for computing distances from the center of data.

PCA

Time-series or dynamic dataset require different approaches for outlier detection. A common practice is to first model the given dataset using an
appropriate time-series modeling technique such as ARIMA or ARX and compute residuals between model prediction and given data.

Dimension Reduction and Latent Variable Methods

Latent variable-based methods reduce process dimensionality by finding these hidden latent variables.
PCA : converts the original (x,y,z) space into a 2-D principal component (PC) space where the 1st PC (PC1) corresponds to the direction of maximum spread/variance in data and the 2nd PC
(PC2) corresponds to the direction with highest variance among all directions orthogonal to 1st PC. 

PCA is also frequently utilized for process visualization. For many applications, two or three PCs are adequate for capturing most of the variability in process data and
therefore, the compressed process data can be visualized with a single plot. Plant operators and engineers use this single plot to find past and current patterns in process data.

Process monitoring/fault detection indices
Hotelling’s T2 and SPE (also called Q) statistics are the two monitoring indices utilized for process monitoring. 

The T2 and Q indexes quantify different kinds of variations in data. While T2 is a measure of distance from origin in PC space, Q is a measure of the portion of data that is not explained
by the PCA model.

Variants of Classical PCA

Dynamic PCA (DPCA), Multiway PCA, Kernel PCA (Kernalized methods have become very attractive for dealing with nonlinear data while retaining the simplicity of their linear counterparts.

PLS : Partial least squares (PLS) is a supervised multivariate regression technique that estimates linear relationship between a set of input variables and a set of output variables. Like PCA,
PLS transforms raw data into latent components - input (X) and output (Y) data matrices are transformed into score matrices T and U, respectively. Figure 2.13 provides a conceptual
comparison of PLS methodology with those of other popular linear regression techniques, principal component regression (PCR) and multivariate linear regression (MLR)


A good rule of thumb when developing soft sensors is to always choose a model with lowest complexity if multiple models with similar predictive
accuracies are available. For PLS and PCR, higher number of latent components imply higher complexity.


Variants of Classical PLS

Dynamic PLS, Kernel PCS

Independent Component Analysis (ICA) is a multivariate technique for transforming measured
variables into statistically independent latent variables in a lower-dimensional space.


ICA and PCA 

IC'a are independent and uncorrelated, non-gaussian, use high order statistics PC's are uncorrelated, gaussian, use second-order statistics, captured variance.

Fisher Discriminant Analysis (FDA), also called linear discriminant analysis (LDA), is a multivariate dimensionality reduction technique which maximizes the ‘separation’ in the lower
dimensional space between data belonging to different classes.

This observation was expected because PCA, while determining the projection directions, does not consider information about different data
classes. Therefore, if your intention is to reduce dimensionality for subsequent data classification and training data is labeled into different classes, then FDA is more suitable.Due to the powerful data discrimination ability of FDA, it is widely used in process industry for
operating mode classification and fault diagnosis. While FDA is a powerful tool for fault diagnosis, it can also be used for fault detection by including data from ‘no-fault’ or normal plant operation as a separate class.

Fault detection refers to the task of determining whether abnormal process conditions have occurred.
Fault identification and fault isolation, both refer to the task of identifying the process variables that exhibit abnormal behavior. Identifying these variables can help in determining the root-cause of the fault. The step of ‘fault diagnosis’ in the PCA
chapter should have been called ‘fault identification’.

Support Vector Machines & Kernel-based Learning

SVM (support vector machine) is one such algorithm which excels in dealing with high-dimensional, nonlinear, and small or medium-sized data. SVMs, by design, minimize
overfitting to provide excellent generalization performance.  SVMs have been employed for fault classification, process monitoring, outlier detection, soft sensing, etc.

SVMs: An Introduction: The classical SVM is a supervised linear technique for solving binary classification problems. 
Hard and soft class classification (Slack variables).

C as regularization hyperparameter
The slack variables not only help find a solution in the presence of gross impurity, but they also help to avoid overfitting noisy data. 
 Trade-off between margin maximization and training accuracy. The hyperparameter C is the knob to control the trade-off. A large value of C implies heavy
penalization of the constraint violations which will prevent misclassifications, while small value of C allows more misclassifications during model fitting for better margin.

The Kernel Trick for Nonlinear Data

These kernel functions allow us to obtain powerful nonlinear classifiers while retaining all the
benefits of the original linear SVM method.

Table below lists some commonly used kernel functions. The first one is simply the familiar
dot product of two vectors, while the 2nd one, RBF (radial basis function) or gaussian kernel
is the most popular (and usually the default choice) kernel for nonlinear problems.

You will notice that Sklearn uses the hyperparameter gamma which is simply 1/2𝜎2

Kernels provide an indirect measure of similarity between any two points in the high-dimensional space.


Support vector data description (SVDD) is the unsupervised form of SVM algorithm used for dealing with problems where training samples belong to only one class and the model
objective is to determine if any test/new sample belongs to the same class of data or not.

now you are convinced of the powerful capabilities of SVM for discriminating between different classes of data and compact bounding of normal operational data. A big
requirement for successful application of SVM is that the training dataset should be very representative of the ‘normal’ dataset and fully characterize all the expected variations.

Support vector regression (SVR) is another variant of SVM used for linear and nonlinear regressions. SVR attempts to find a regression curve that is as close as possible to the training
observations. Geometrically, as shown in Figure 7.11 for kernelized-SVR, a tube of pre- specified width (𝜀) is fitted to the data and any sample lying outside the tube is penalized. You
will see later that SVR’s optimization program is designed to obtain a good balance between model generalization capabilities and the ability to fit the training data.

SVR has been found to provide performance superior to ANNs for small and medium-sized high-dimensional datasets

SVR and ANN

SVMs are in a league of their own and are well-suited for industrial processes with difficult to estimate process parameters. With
elegant mathematical background, just a few hyperparameters, excellent generalization capabilities, and guaranteed unique global optimum, SVMs are among the best ML
algorithms.

Finding Groups in Process Data: Clustering & Mixture Modeling

The Conventional PCA-based monitoring, on the other hand, would fail to identify the outlier. And in the correlation between the variables is different in the two clusters. From soft
sensing perspective, it would make sense to build separate models for the two clusters. The Conventional PLS model would give inaccurate results.

Clustering algorithms:
• Centroid-based algorithms: In these algorithms, the similarity between data-points is quantified by the distance of the data-points from the centroid of the clusters. K-Means,
Fuzzy C-Means models belong to this category.
• Distribution-based algorithms: Here, similarity between data-points is quantified by computing the probabilities of the data-points belonging to the same underlying
distribution. Gaussian mixture models (GMMs) are popular in this category.
• Density-based algorithms: Density models find areas of high density of data in the multivariable space and assign data-points to different clusters/regions. DBSCAN and
OPTICS are popular examples of density models.
• Connectivity-based algorithms: In these models, similarity is quantified by directly measuring the distance between the data-points. Hierarchical clustering models belong to
this category.

 Performing PCA serves other purposes as well. We will see later that expectation-maximization (EM) algorithm is employed to estimate cluster
parameters in K-Means and GMM models. High dimensionality implies high number of parameters to be estimated which increases possibility of EM converging to locally optimum
results and correlated variables causes EM convergence issues. PCA helps to overcome these two problems simultaneously.

K-means: he cluster assignment of the data points is determined such that the following sum of squared errors, also called cluster inertia, is
minimized.

 For high-dimensional data, it may not always be possible to project most of the data variability onto 2 or 3 PCs and thus it becomes difficult to judge the goodness of clustering or visualize
the clusters using 2D or 3D plots.Silhouette plots can be used to visualize and quantify cluster quality. Note that silhouette analysis is not specific to k-means and can
be applied to study any clustering result. Silhouette coefficient or value of a data-point ranges from -1 to 1 and is a measure of how far
the data-point is from data-points in neighboring cluster as compared to data-points in the same cluster. A value of 1 indicates that the data-point is far away from the neighboring
cluster and values close to 0 indicate that the data-point is close to the boundary between the two clusters. Negative values indicate wrong cluster assignment.

DBSCAN, a Density-based algorithm, can easily handle irregularly shaped data distributions.
The ability to deal with arbitrarily shaped data, robustness to noise and outliers make DBSCAN well-suited for clustering process data.

DBSCAN works by grouping together data-points that form regions of high data densities. Specifically, each data-point is classified into one of the following 3 categories:
• A core point if there are more than a specified number (minPts) of data-points within a
specified distance (𝜺)
• A border point if less than minPts data-points lie within its 𝜺 neighborhood, but the
data-point itself lies within the 𝜺 neighborhood of another core point
• A noise point if it isn’t classified as either core or border point

A common disadvantage of both k-means and DBSCAN is that the data-points are ‘hard-clustered’ (each data point assigned to exactly one cluster). For
example, consider the data-points categorized as noise-points in Figure 8.10. Although these points are far away from cluster center, they may still correspond to normal operating
conditions representing low-likelihood instances. Throwing away these noise points can lead to loss of crucial information and may result in improperly set monitoring thresholds.
Additionally, in some cases, hard clustering may not be obvious if a data-point lies equidistant from different clusters. Soft-clustering algorithms can be used to solve these problems as
these algorithms provide some measure of association of data-points with different clusters. We will study one such popular algorithm, gaussian mixture model.

Hard clustering = each point in only one group. Soft clustering = each point can belong to multiple groups with different probabilities.

Determining the number of clusters
One of the limitations with EM algorithm for GMM modeling is that the number of gaussian components is assumed to be known beforehand. This may not always be true. To overcome this,
we can use a method similar to elbow method that we used for k-means. While in k- means, we plotted SSEs for different number of clusters, in GMM we will utilize the Bayesian
Information Criterion (BIC) metric (introduced in Chapter 3). Unlike SSEs, BICs increase after a while and optimal gaussian components is the value that minimizes BIC.


